{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c7a514-6e29-4d0f-b347-aa2c3ff3f474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 18:18:18 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10                     Off |   00000000:00:08.0 Off |                    0 |\n",
      "|  0%   26C    P8              9W /  150W |      14MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2519      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ba5ff-d11c-4c08-a515-297f8cf7870c",
   "metadata": {},
   "source": [
    "前期已经训练了4000步，从4000步开始继续训练。4000步时，training loss 在3.4左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d055603-f0b1-4449-bfa7-6ce875a085f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [01:34<00:00, 13.48s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 592352.24 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 289841.47 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 457669.31 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:02<00:00, 46696.28 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1840.07 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1891.23 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "resume checkpoint from  checkpoint-4000\n",
      "Loading model from /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-4000.\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 50 (from args) != 10 (from trainer_state.json)\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 4000\n",
      "  Will skip the first 0 epochs then the first 4000 batches in the first epoch.\n",
      "{'loss': 3.2873, 'grad_norm': 10.63883113861084, 'learning_rate': 2.135714285714286e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3691, 'grad_norm': 8.334632873535156, 'learning_rate': 2.1285714285714286e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4869, 'grad_norm': 8.754945755004883, 'learning_rate': 2.1214285714285713e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4018, 'grad_norm': 9.554681777954102, 'learning_rate': 2.1142857142857144e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2859, 'grad_norm': 8.533652305603027, 'learning_rate': 2.107142857142857e-05, 'epoch': 0.14}\n",
      "{'loss': 3.332, 'grad_norm': 9.182503700256348, 'learning_rate': 2.1e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2113, 'grad_norm': 7.829322814941406, 'learning_rate': 2.092857142857143e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4262, 'grad_norm': 8.769614219665527, 'learning_rate': 2.0857142857142857e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4561, 'grad_norm': 9.670257568359375, 'learning_rate': 2.0785714285714288e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3584, 'grad_norm': 9.090179443359375, 'learning_rate': 2.0714285714285718e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3854, 'grad_norm': 10.532957077026367, 'learning_rate': 2.0642857142857146e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3525, 'grad_norm': 8.48774242401123, 'learning_rate': 2.0571428571428573e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4459, 'grad_norm': 7.918741226196289, 'learning_rate': 2.05e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3098, 'grad_norm': 8.011043548583984, 'learning_rate': 2.042857142857143e-05, 'epoch': 0.14}\n",
      "{'loss': 3.366, 'grad_norm': 8.88237476348877, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3639, 'grad_norm': 8.929473876953125, 'learning_rate': 2.0285714285714286e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2811, 'grad_norm': 9.712865829467773, 'learning_rate': 2.0214285714285716e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3566, 'grad_norm': 8.496870040893555, 'learning_rate': 2.0142857142857144e-05, 'epoch': 0.15}\n",
      "{'loss': 3.5063, 'grad_norm': 9.923656463623047, 'learning_rate': 2.007142857142857e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4, 'grad_norm': 8.26358699798584, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 3.357, 'grad_norm': 7.7765421867370605, 'learning_rate': 1.992857142857143e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3504, 'grad_norm': 8.668510437011719, 'learning_rate': 1.9857142857142856e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3287, 'grad_norm': 8.666702270507812, 'learning_rate': 1.9785714285714287e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4982, 'grad_norm': 8.227205276489258, 'learning_rate': 1.9714285714285714e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4244, 'grad_norm': 8.091066360473633, 'learning_rate': 1.9642857142857145e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3736, 'grad_norm': 8.732124328613281, 'learning_rate': 1.9571428571428572e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2648, 'grad_norm': 9.597515106201172, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2002, 'grad_norm': 8.528071403503418, 'learning_rate': 1.942857142857143e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3219, 'grad_norm': 8.071585655212402, 'learning_rate': 1.9357142857142858e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4146, 'grad_norm': 8.6820068359375, 'learning_rate': 1.928571428571429e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4246, 'grad_norm': 8.360799789428711, 'learning_rate': 1.9214285714285716e-05, 'epoch': 0.15}\n",
      "{'loss': 3.209, 'grad_norm': 8.397856712341309, 'learning_rate': 1.9142857142857143e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2453, 'grad_norm': 9.072649955749512, 'learning_rate': 1.9071428571428574e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3713, 'grad_norm': 9.046819686889648, 'learning_rate': 1.9e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3184, 'grad_norm': 8.647790908813477, 'learning_rate': 1.892857142857143e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3113, 'grad_norm': 9.785033226013184, 'learning_rate': 1.885714285714286e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3838, 'grad_norm': 9.074071884155273, 'learning_rate': 1.8785714285714286e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4113, 'grad_norm': 8.613424301147461, 'learning_rate': 1.8714285714285714e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2232, 'grad_norm': 7.807788372039795, 'learning_rate': 1.864285714285714e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3463, 'grad_norm': 8.414359092712402, 'learning_rate': 1.8571428571428572e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3061, 'grad_norm': 9.91424560546875, 'learning_rate': 1.85e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3619, 'grad_norm': 8.430754661560059, 'learning_rate': 1.842857142857143e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4393, 'grad_norm': 9.330479621887207, 'learning_rate': 1.835714285714286e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1514, 'grad_norm': 8.756885528564453, 'learning_rate': 1.8285714285714288e-05, 'epoch': 0.15}\n",
      "{'loss': 3.284, 'grad_norm': 8.920645713806152, 'learning_rate': 1.8214285714285715e-05, 'epoch': 0.16}\n",
      "{'loss': 3.383, 'grad_norm': 9.355978012084961, 'learning_rate': 1.8142857142857146e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4688, 'grad_norm': 9.73287296295166, 'learning_rate': 1.8071428571428573e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3895, 'grad_norm': 7.897879123687744, 'learning_rate': 1.8e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3344, 'grad_norm': 9.52852725982666, 'learning_rate': 1.792857142857143e-05, 'epoch': 0.16}\n",
      "{'loss': 3.408, 'grad_norm': 9.231059074401855, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.16}\n",
      " 64%|█████████████████████████              | 4500/7000 [07:32<40:27,  1.03it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.90s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.98s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.09s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.500 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.873520000000003, 'eval_rouge-2': 6.84427, 'eval_rouge-l': 25.070546, 'eval_bleu-4': 0.03563447702393692, 'eval_runtime': 58.0667, 'eval_samples_per_second': 0.861, 'eval_steps_per_second': 0.069, 'epoch': 0.16}\n",
      " 64%|█████████████████████████              | 4500/7000 [08:30<40:27,  1.03it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.09s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-4500\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4729, 'grad_norm': 9.173454284667969, 'learning_rate': 1.7785714285714286e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2229, 'grad_norm': 10.417078018188477, 'learning_rate': 1.7714285714285713e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3957, 'grad_norm': 10.209554672241211, 'learning_rate': 1.7642857142857144e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3656, 'grad_norm': 8.738326072692871, 'learning_rate': 1.757142857142857e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2145, 'grad_norm': 8.8156156539917, 'learning_rate': 1.75e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3668, 'grad_norm': 10.350988388061523, 'learning_rate': 1.742857142857143e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4236, 'grad_norm': 8.676497459411621, 'learning_rate': 1.7357142857142856e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2771, 'grad_norm': 8.866520881652832, 'learning_rate': 1.7285714285714287e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3664, 'grad_norm': 10.030186653137207, 'learning_rate': 1.7214285714285715e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3154, 'grad_norm': 9.869388580322266, 'learning_rate': 1.7142857142857145e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2676, 'grad_norm': 9.915884017944336, 'learning_rate': 1.7071428571428573e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4332, 'grad_norm': 8.817004203796387, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3484, 'grad_norm': 10.484235763549805, 'learning_rate': 1.692857142857143e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3898, 'grad_norm': 9.414162635803223, 'learning_rate': 1.6857142857142858e-05, 'epoch': 0.16}\n",
      "{'loss': 3.335, 'grad_norm': 8.493265151977539, 'learning_rate': 1.6785714285714285e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4221, 'grad_norm': 8.869502067565918, 'learning_rate': 1.6714285714285716e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4061, 'grad_norm': 10.534095764160156, 'learning_rate': 1.6642857142857143e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5459, 'grad_norm': 9.511456489562988, 'learning_rate': 1.657142857142857e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3723, 'grad_norm': 9.43789005279541, 'learning_rate': 1.65e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5209, 'grad_norm': 9.233786582946777, 'learning_rate': 1.642857142857143e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4574, 'grad_norm': 9.563425064086914, 'learning_rate': 1.6357142857142856e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3197, 'grad_norm': 9.443870544433594, 'learning_rate': 1.6285714285714287e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4518, 'grad_norm': 7.900263786315918, 'learning_rate': 1.6214285714285714e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2744, 'grad_norm': 8.569753646850586, 'learning_rate': 1.614285714285714e-05, 'epoch': 0.17}\n",
      "{'loss': 3.432, 'grad_norm': 9.714179039001465, 'learning_rate': 1.6071428571428572e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3346, 'grad_norm': 8.301942825317383, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3432, 'grad_norm': 9.321069717407227, 'learning_rate': 1.592857142857143e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3541, 'grad_norm': 8.906538963317871, 'learning_rate': 1.5857142857142857e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4385, 'grad_norm': 9.680598258972168, 'learning_rate': 1.5785714285714288e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4207, 'grad_norm': 9.483654975891113, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5938, 'grad_norm': 9.353586196899414, 'learning_rate': 1.5642857142857143e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2832, 'grad_norm': 10.500126838684082, 'learning_rate': 1.5571428571428573e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2094, 'grad_norm': 8.529848098754883, 'learning_rate': 1.55e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2672, 'grad_norm': 8.72714900970459, 'learning_rate': 1.5428571428571428e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3619, 'grad_norm': 10.751246452331543, 'learning_rate': 1.535714285714286e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2145, 'grad_norm': 8.8012056350708, 'learning_rate': 1.5285714285714286e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2828, 'grad_norm': 8.260388374328613, 'learning_rate': 1.5214285714285715e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4092, 'grad_norm': 8.983519554138184, 'learning_rate': 1.5142857142857144e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4377, 'grad_norm': 7.9867262840271, 'learning_rate': 1.5071428571428573e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3197, 'grad_norm': 9.205516815185547, 'learning_rate': 1.5e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2666, 'grad_norm': 8.476239204406738, 'learning_rate': 1.4928571428571431e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4059, 'grad_norm': 8.88834285736084, 'learning_rate': 1.4857142857142858e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2195, 'grad_norm': 9.740540504455566, 'learning_rate': 1.4785714285714286e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3449, 'grad_norm': 8.972967147827148, 'learning_rate': 1.4714285714285713e-05, 'epoch': 0.17}\n",
      "{'loss': 3.333, 'grad_norm': 9.821104049682617, 'learning_rate': 1.4642857142857144e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4604, 'grad_norm': 8.887053489685059, 'learning_rate': 1.4571428571428573e-05, 'epoch': 0.17}\n",
      "{'loss': 3.335, 'grad_norm': 9.023198127746582, 'learning_rate': 1.45e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3238, 'grad_norm': 9.565814018249512, 'learning_rate': 1.442857142857143e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2568, 'grad_norm': 9.650924682617188, 'learning_rate': 1.4357142857142858e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1947, 'grad_norm': 8.352428436279297, 'learning_rate': 1.4285714285714285e-05, 'epoch': 0.17}\n",
      " 71%|███████████████████████████▊           | 5000/7000 [15:59<28:29,  1.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.90s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 18.00s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.751559999999994, 'eval_rouge-2': 7.200095999999999, 'eval_rouge-l': 24.151291999999998, 'eval_bleu-4': 0.031163615897775453, 'eval_runtime': 79.256, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.05, 'epoch': 0.17}\n",
      " 71%|███████████████████████████▊           | 5000/7000 [17:18<28:29,  1.17it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.10s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-5000\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4322, 'grad_norm': 9.732316017150879, 'learning_rate': 1.4214285714285716e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3121, 'grad_norm': 9.743104934692383, 'learning_rate': 1.4142857142857143e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3275, 'grad_norm': 8.082097053527832, 'learning_rate': 1.407142857142857e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2885, 'grad_norm': 10.244769096374512, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3025, 'grad_norm': 10.35363483428955, 'learning_rate': 1.392857142857143e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4221, 'grad_norm': 10.13546371459961, 'learning_rate': 1.3857142857142858e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4162, 'grad_norm': 8.139191627502441, 'learning_rate': 1.3785714285714285e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3922, 'grad_norm': 8.844154357910156, 'learning_rate': 1.3714285714285716e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4344, 'grad_norm': 7.986021041870117, 'learning_rate': 1.3642857142857143e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3342, 'grad_norm': 8.323134422302246, 'learning_rate': 1.357142857142857e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3338, 'grad_norm': 8.450142860412598, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3602, 'grad_norm': 8.550219535827637, 'learning_rate': 1.3428571428571429e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3578, 'grad_norm': 9.31253433227539, 'learning_rate': 1.3357142857142858e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1877, 'grad_norm': 9.471603393554688, 'learning_rate': 1.3285714285714288e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3572, 'grad_norm': 9.529119491577148, 'learning_rate': 1.3214285714285716e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3113, 'grad_norm': 9.973806381225586, 'learning_rate': 1.3142857142857143e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3824, 'grad_norm': 9.759979248046875, 'learning_rate': 1.3071428571428574e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2674, 'grad_norm': 8.64293384552002, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3799, 'grad_norm': 9.238280296325684, 'learning_rate': 1.2928571428571428e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4088, 'grad_norm': 8.713932991027832, 'learning_rate': 1.2857142857142857e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4166, 'grad_norm': 10.51132869720459, 'learning_rate': 1.2785714285714286e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3668, 'grad_norm': 9.315122604370117, 'learning_rate': 1.2714285714285715e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3527, 'grad_norm': 9.495963096618652, 'learning_rate': 1.2642857142857143e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4756, 'grad_norm': 8.4010009765625, 'learning_rate': 1.2571428571428573e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2723, 'grad_norm': 8.41867446899414, 'learning_rate': 1.25e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2426, 'grad_norm': 9.884780883789062, 'learning_rate': 1.242857142857143e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2145, 'grad_norm': 9.997832298278809, 'learning_rate': 1.2357142857142857e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1102, 'grad_norm': 8.511327743530273, 'learning_rate': 1.2285714285714286e-05, 'epoch': 0.18}\n",
      " 76%|█████████████████████████████▍         | 5289/7000 [21:40<26:09,  1.09it/s]"
     ]
    }
   ],
   "source": [
    "! python3 /home/abc/mydisk/ChatGLM3/finetune_demo/finetune_hf.py /home/abc/mydisk/ChatGLM3/finetune_demo/data/ /home/abc/mydisk/model/chatglm3-6b/ /home/abc/mydisk/ChatGLM3/finetune_demo/configs/lora.yaml YES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441a095-c9ee-4155-a41a-d26017c7dc63",
   "metadata": {},
   "source": [
    "由于vnc断开，重连后继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1ebca8-892b-4ce3-96d2-5006febe44be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [01:34<00:00, 13.48s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1853.62 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "resume checkpoint from  checkpoint-6000\n",
      "Loading model from /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-6000.\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 50 (from args) != 10 (from trainer_state.json)\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 6000\n",
      "  Will skip the first 0 epochs then the first 6000 batches in the first epoch.\n",
      "{'loss': 3.3201, 'grad_norm': 8.9242525100708, 'learning_rate': 7.071428571428572e-06, 'epoch': 0.21}\n",
      "{'loss': 3.276, 'grad_norm': 9.534913063049316, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.21}\n",
      "{'loss': 3.1863, 'grad_norm': 9.426773071289062, 'learning_rate': 6.928571428571429e-06, 'epoch': 0.21}\n",
      "{'loss': 3.2396, 'grad_norm': 10.660757064819336, 'learning_rate': 6.857142857142858e-06, 'epoch': 0.21}\n",
      "{'loss': 3.426, 'grad_norm': 9.195355415344238, 'learning_rate': 6.785714285714285e-06, 'epoch': 0.21}\n",
      "{'loss': 3.2604, 'grad_norm': 10.662882804870605, 'learning_rate': 6.714285714285714e-06, 'epoch': 0.21}\n",
      "{'loss': 3.374, 'grad_norm': 9.040611267089844, 'learning_rate': 6.642857142857144e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3293, 'grad_norm': 9.960033416748047, 'learning_rate': 6.5714285714285714e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3158, 'grad_norm': 8.965798377990723, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.21}\n",
      "{'loss': 3.4223, 'grad_norm': 9.95106315612793, 'learning_rate': 6.428571428571429e-06, 'epoch': 0.21}\n",
      "{'loss': 3.2537, 'grad_norm': 10.026883125305176, 'learning_rate': 6.357142857142858e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3441, 'grad_norm': 11.481048583984375, 'learning_rate': 6.285714285714287e-06, 'epoch': 0.21}\n",
      "{'loss': 3.2252, 'grad_norm': 9.396931648254395, 'learning_rate': 6.214285714285715e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3725, 'grad_norm': 8.630270004272461, 'learning_rate': 6.142857142857143e-06, 'epoch': 0.21}\n",
      "{'loss': 3.4453, 'grad_norm': 10.110384941101074, 'learning_rate': 6.071428571428572e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3373, 'grad_norm': 8.889254570007324, 'learning_rate': 6e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3195, 'grad_norm': 9.315354347229004, 'learning_rate': 5.928571428571429e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3877, 'grad_norm': 9.201800346374512, 'learning_rate': 5.857142857142857e-06, 'epoch': 0.22}\n",
      "{'loss': 3.2367, 'grad_norm': 8.732199668884277, 'learning_rate': 5.785714285714286e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3014, 'grad_norm': 9.63902759552002, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.22}\n",
      "{'loss': 3.1979, 'grad_norm': 9.674988746643066, 'learning_rate': 5.642857142857143e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4014, 'grad_norm': 11.196784019470215, 'learning_rate': 5.571428571428572e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3533, 'grad_norm': 10.81622314453125, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4014, 'grad_norm': 9.159246444702148, 'learning_rate': 5.428571428571429e-06, 'epoch': 0.22}\n",
      "{'loss': 3.2826, 'grad_norm': 10.383362770080566, 'learning_rate': 5.357142857142857e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4252, 'grad_norm': 9.46993350982666, 'learning_rate': 5.285714285714286e-06, 'epoch': 0.22}\n",
      "{'loss': 3.2826, 'grad_norm': 8.285722732543945, 'learning_rate': 5.214285714285714e-06, 'epoch': 0.22}\n",
      "{'loss': 3.2699, 'grad_norm': 9.674281120300293, 'learning_rate': 5.142857142857143e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3828, 'grad_norm': 9.30588150024414, 'learning_rate': 5.071428571428571e-06, 'epoch': 0.22}\n",
      "{'loss': 3.2879, 'grad_norm': 9.482057571411133, 'learning_rate': 5e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4227, 'grad_norm': 9.888453483581543, 'learning_rate': 4.9285714285714286e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3676, 'grad_norm': 9.408978462219238, 'learning_rate': 4.857142857142858e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3561, 'grad_norm': 10.381526947021484, 'learning_rate': 4.785714285714286e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3979, 'grad_norm': 10.481005668640137, 'learning_rate': 4.714285714285715e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3609, 'grad_norm': 11.496609687805176, 'learning_rate': 4.642857142857143e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4975, 'grad_norm': 10.540993690490723, 'learning_rate': 4.571428571428572e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3848, 'grad_norm': 8.839640617370605, 'learning_rate': 4.5e-06, 'epoch': 0.22}\n",
      "{'loss': 3.5086, 'grad_norm': 8.737637519836426, 'learning_rate': 4.428571428571428e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3988, 'grad_norm': 10.313472747802734, 'learning_rate': 4.357142857142857e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3396, 'grad_norm': 8.843329429626465, 'learning_rate': 4.285714285714286e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3432, 'grad_norm': 9.277091979980469, 'learning_rate': 4.2142857142857145e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3658, 'grad_norm': 10.111001968383789, 'learning_rate': 4.142857142857143e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3605, 'grad_norm': 9.740578651428223, 'learning_rate': 4.071428571428572e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4406, 'grad_norm': 8.845909118652344, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3322, 'grad_norm': 8.801129341125488, 'learning_rate': 3.928571428571429e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2834, 'grad_norm': 9.179667472839355, 'learning_rate': 3.857142857142857e-06, 'epoch': 0.23}\n",
      "{'loss': 3.4248, 'grad_norm': 9.223170280456543, 'learning_rate': 3.785714285714286e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3955, 'grad_norm': 9.10152816772461, 'learning_rate': 3.7142857142857146e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3752, 'grad_norm': 9.685290336608887, 'learning_rate': 3.642857142857143e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3148, 'grad_norm': 9.461128234863281, 'learning_rate': 3.5714285714285714e-06, 'epoch': 0.23}\n",
      " 93%|████████████████████████████████████▏  | 6500/7000 [07:34<07:02,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.88s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.94s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:52<00:00, 12.02s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.492 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.944836000000006, 'eval_rouge-2': 7.002568000000001, 'eval_rouge-l': 23.977282, 'eval_bleu-4': 0.03434403167555024, 'eval_runtime': 57.5503, 'eval_samples_per_second': 0.869, 'eval_steps_per_second': 0.07, 'epoch': 0.23}\n",
      " 93%|████████████████████████████████████▏  | 6500/7000 [08:32<07:02,  1.18it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.02s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-6500\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2777, 'grad_norm': 9.855107307434082, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.23}\n",
      "{'loss': 3.399, 'grad_norm': 11.238696098327637, 'learning_rate': 3.428571428571429e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2891, 'grad_norm': 8.7227201461792, 'learning_rate': 3.357142857142857e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2766, 'grad_norm': 8.83281135559082, 'learning_rate': 3.2857142857142857e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3488, 'grad_norm': 9.44653034210205, 'learning_rate': 3.2142857142857143e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2707, 'grad_norm': 9.793712615966797, 'learning_rate': 3.1428571428571433e-06, 'epoch': 0.23}\n",
      "{'loss': 3.4109, 'grad_norm': 8.833447456359863, 'learning_rate': 3.0714285714285715e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3979, 'grad_norm': 8.54694652557373, 'learning_rate': 3e-06, 'epoch': 0.23}\n",
      "{'loss': 3.4645, 'grad_norm': 8.4360990524292, 'learning_rate': 2.9285714285714287e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3881, 'grad_norm': 9.814752578735352, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2658, 'grad_norm': 10.8184175491333, 'learning_rate': 2.785714285714286e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3436, 'grad_norm': 8.798362731933594, 'learning_rate': 2.7142857142857144e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3004, 'grad_norm': 9.863048553466797, 'learning_rate': 2.642857142857143e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2801, 'grad_norm': 9.216781616210938, 'learning_rate': 2.5714285714285716e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2783, 'grad_norm': 10.658056259155273, 'learning_rate': 2.5e-06, 'epoch': 0.23}\n",
      "{'loss': 3.241, 'grad_norm': 10.955062866210938, 'learning_rate': 2.428571428571429e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3219, 'grad_norm': 9.487701416015625, 'learning_rate': 2.3571428571428574e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2805, 'grad_norm': 8.877601623535156, 'learning_rate': 2.285714285714286e-06, 'epoch': 0.23}\n",
      "{'loss': 3.359, 'grad_norm': 10.448278427124023, 'learning_rate': 2.214285714285714e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2965, 'grad_norm': 8.918591499328613, 'learning_rate': 2.142857142857143e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3674, 'grad_norm': 10.149796485900879, 'learning_rate': 2.0714285714285713e-06, 'epoch': 0.23}\n",
      "{'loss': 3.4268, 'grad_norm': 9.733644485473633, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3848, 'grad_norm': 14.625615119934082, 'learning_rate': 1.9285714285714285e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2838, 'grad_norm': 8.526371955871582, 'learning_rate': 1.8571428571428573e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3453, 'grad_norm': 9.871088027954102, 'learning_rate': 1.7857142857142857e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3477, 'grad_norm': 11.236716270446777, 'learning_rate': 1.7142857142857145e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3588, 'grad_norm': 9.584877967834473, 'learning_rate': 1.6428571428571429e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3256, 'grad_norm': 9.769596099853516, 'learning_rate': 1.5714285714285717e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2916, 'grad_norm': 9.423663139343262, 'learning_rate': 1.5e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2707, 'grad_norm': 9.866314888000488, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.24}\n",
      "{'loss': 3.4047, 'grad_norm': 9.476665496826172, 'learning_rate': 1.3571428571428572e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3885, 'grad_norm': 12.23911190032959, 'learning_rate': 1.2857142857142858e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3422, 'grad_norm': 9.161087036132812, 'learning_rate': 1.2142857142857144e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3109, 'grad_norm': 10.371554374694824, 'learning_rate': 1.142857142857143e-06, 'epoch': 0.24}\n",
      "{'loss': 3.4865, 'grad_norm': 9.688421249389648, 'learning_rate': 1.0714285714285716e-06, 'epoch': 0.24}\n",
      "{'loss': 3.393, 'grad_norm': 9.454866409301758, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3379, 'grad_norm': 9.703989028930664, 'learning_rate': 9.285714285714287e-07, 'epoch': 0.24}\n",
      "{'loss': 3.2857, 'grad_norm': 9.502431869506836, 'learning_rate': 8.571428571428572e-07, 'epoch': 0.24}\n",
      "{'loss': 3.1424, 'grad_norm': 9.669692039489746, 'learning_rate': 7.857142857142858e-07, 'epoch': 0.24}\n",
      "{'loss': 3.2992, 'grad_norm': 10.258475303649902, 'learning_rate': 7.142857142857143e-07, 'epoch': 0.24}\n",
      "{'loss': 3.441, 'grad_norm': 9.464599609375, 'learning_rate': 6.428571428571429e-07, 'epoch': 0.24}\n",
      "{'loss': 3.2, 'grad_norm': 9.508779525756836, 'learning_rate': 5.714285714285715e-07, 'epoch': 0.24}\n",
      "{'loss': 3.383, 'grad_norm': 8.60346508026123, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.24}\n",
      "{'loss': 3.2746, 'grad_norm': 10.588648796081543, 'learning_rate': 4.285714285714286e-07, 'epoch': 0.24}\n",
      "{'loss': 3.215, 'grad_norm': 9.108033180236816, 'learning_rate': 3.5714285714285716e-07, 'epoch': 0.24}\n",
      "{'loss': 3.3805, 'grad_norm': 10.754148483276367, 'learning_rate': 2.8571428571428575e-07, 'epoch': 0.24}\n",
      "{'loss': 3.4625, 'grad_norm': 10.603134155273438, 'learning_rate': 2.142857142857143e-07, 'epoch': 0.24}\n",
      "{'loss': 3.2445, 'grad_norm': 9.960352897644043, 'learning_rate': 1.4285714285714287e-07, 'epoch': 0.24}\n",
      "{'loss': 3.3443, 'grad_norm': 10.099071502685547, 'learning_rate': 7.142857142857144e-08, 'epoch': 0.24}\n",
      "{'loss': 3.3594, 'grad_norm': 8.903082847595215, 'learning_rate': 0.0, 'epoch': 0.24}\n",
      "100%|███████████████████████████████████████| 7000/7000 [16:03<00:00,  1.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.37s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:11, 11.83s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.152576, 'eval_rouge-2': 6.815041999999998, 'eval_rouge-l': 24.314358, 'eval_bleu-4': 0.0350063458012163, 'eval_runtime': 51.9233, 'eval_samples_per_second': 0.963, 'eval_steps_per_second': 0.077, 'epoch': 0.24}\n",
      "100%|███████████████████████████████████████| 7000/7000 [16:55<00:00,  1.09it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 14.14s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-7000\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1016.009, 'train_samples_per_second': 27.559, 'train_steps_per_second': 6.89, 'train_loss': 0.4770245535714286, 'epoch': 0.24}\n",
      "100%|███████████████████████████████████████| 7000/7000 [16:56<00:00,  6.89it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [17:06<00:00, 15.33s/it]\n"
     ]
    }
   ],
   "source": [
    "! python3 /home/abc/mydisk/ChatGLM3/finetune_demo/finetune_hf.py /home/abc/mydisk/ChatGLM3/finetune_demo/data/ /home/abc/mydisk/model/chatglm3-6b/ /home/abc/mydisk/ChatGLM3/finetune_demo/configs/lora.yaml YES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59a078-ff73-4330-acaa-b9ef676bd760",
   "metadata": {},
   "source": [
    "已训练7000步，train loss整体在3.2~3.3之间徘徊，再训练3000步，观察loss变化趋势，logging step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ba5108-f385-4460-9b2c-25f291331441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [01:34<00:00, 13.48s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "resume checkpoint from  checkpoint-7000\n",
      "Loading model from /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-7000.\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 7000\n",
      "  Will skip the first 0 epochs then the first 7000 batches in the first epoch.\n",
      "{'loss': 3.6125, 'grad_norm': 9.977222442626953, 'learning_rate': 1.4950000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4523, 'grad_norm': 8.99509334564209, 'learning_rate': 1.49e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2174, 'grad_norm': 10.480881690979004, 'learning_rate': 1.485e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1123, 'grad_norm': 9.126829147338867, 'learning_rate': 1.48e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3664, 'grad_norm': 8.87652587890625, 'learning_rate': 1.475e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2506, 'grad_norm': 9.197914123535156, 'learning_rate': 1.47e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3936, 'grad_norm': 9.88743782043457, 'learning_rate': 1.465e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3707, 'grad_norm': 10.288954734802246, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3227, 'grad_norm': 9.229057312011719, 'learning_rate': 1.455e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3432, 'grad_norm': 8.966547012329102, 'learning_rate': 1.45e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3523, 'grad_norm': 8.81015682220459, 'learning_rate': 1.4449999999999999e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3285, 'grad_norm': 10.292404174804688, 'learning_rate': 1.44e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3918, 'grad_norm': 9.577080726623535, 'learning_rate': 1.435e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4131, 'grad_norm': 9.783323287963867, 'learning_rate': 1.43e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3135, 'grad_norm': 8.579766273498535, 'learning_rate': 1.4249999999999999e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3492, 'grad_norm': 9.966055870056152, 'learning_rate': 1.42e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3707, 'grad_norm': 9.127089500427246, 'learning_rate': 1.415e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3979, 'grad_norm': 9.673480033874512, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4158, 'grad_norm': 10.85904598236084, 'learning_rate': 1.4050000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2529, 'grad_norm': 8.692190170288086, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2221, 'grad_norm': 9.299792289733887, 'learning_rate': 1.3950000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2717, 'grad_norm': 10.542679786682129, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 3.084, 'grad_norm': 9.625852584838867, 'learning_rate': 1.3850000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3994, 'grad_norm': 10.658276557922363, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4156, 'grad_norm': 9.49113655090332, 'learning_rate': 1.3750000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3674, 'grad_norm': 11.385214805603027, 'learning_rate': 1.3700000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2898, 'grad_norm': 9.549113273620605, 'learning_rate': 1.3650000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3945, 'grad_norm': 9.041598320007324, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3865, 'grad_norm': 10.021004676818848, 'learning_rate': 1.3550000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2936, 'grad_norm': 10.396117210388184, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3553, 'grad_norm': 10.328021049499512, 'learning_rate': 1.3450000000000002e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4102, 'grad_norm': 9.77330207824707, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4377, 'grad_norm': 10.863971710205078, 'learning_rate': 1.3350000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.325, 'grad_norm': 9.452963829040527, 'learning_rate': 1.3300000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2984, 'grad_norm': 9.729535102844238, 'learning_rate': 1.3250000000000002e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2305, 'grad_norm': 9.978012084960938, 'learning_rate': 1.32e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2898, 'grad_norm': 9.510751724243164, 'learning_rate': 1.3150000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.283, 'grad_norm': 9.171337127685547, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3152, 'grad_norm': 11.352823257446289, 'learning_rate': 1.305e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2848, 'grad_norm': 10.275060653686523, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4547, 'grad_norm': 10.680765151977539, 'learning_rate': 1.2950000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3572, 'grad_norm': 10.104523658752441, 'learning_rate': 1.29e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2625, 'grad_norm': 9.259272575378418, 'learning_rate': 1.285e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4486, 'grad_norm': 10.02934741973877, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2814, 'grad_norm': 8.94819164276123, 'learning_rate': 1.2750000000000002e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2398, 'grad_norm': 10.693252563476562, 'learning_rate': 1.27e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2605, 'grad_norm': 9.845727920532227, 'learning_rate': 1.2650000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3404, 'grad_norm': 8.662589073181152, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2826, 'grad_norm': 9.304823875427246, 'learning_rate': 1.255e-05, 'epoch': 0.26}\n",
      "{'loss': 3.0691, 'grad_norm': 9.737397193908691, 'learning_rate': 1.25e-05, 'epoch': 0.26}\n",
      " 75%|████████████████████████████▌         | 7500/10000 [07:29<33:06,  1.26it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.91s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:12, 12.15s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 14.33s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.492 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.535892000000004, 'eval_rouge-2': 7.082702, 'eval_rouge-l': 24.22245, 'eval_bleu-4': 0.03286316395773637, 'eval_runtime': 75.5036, 'eval_samples_per_second': 0.662, 'eval_steps_per_second': 0.053, 'epoch': 0.26}\n",
      " 75%|████████████████████████████▌         | 7500/10000 [08:45<33:06,  1.26it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:49<00:00, 14.33s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-7500\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2785, 'grad_norm': 9.161434173583984, 'learning_rate': 1.2450000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3752, 'grad_norm': 8.821524620056152, 'learning_rate': 1.24e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3283, 'grad_norm': 8.34750747680664, 'learning_rate': 1.235e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4279, 'grad_norm': 10.236010551452637, 'learning_rate': 1.23e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4203, 'grad_norm': 10.35968017578125, 'learning_rate': 1.225e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3787, 'grad_norm': 10.142580032348633, 'learning_rate': 1.22e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3221, 'grad_norm': 10.551435470581055, 'learning_rate': 1.215e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4021, 'grad_norm': 9.307974815368652, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3594, 'grad_norm': 8.688485145568848, 'learning_rate': 1.205e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4635, 'grad_norm': 9.540308952331543, 'learning_rate': 1.2e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3463, 'grad_norm': 10.475468635559082, 'learning_rate': 1.195e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2393, 'grad_norm': 10.697955131530762, 'learning_rate': 1.19e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3508, 'grad_norm': 11.112382888793945, 'learning_rate': 1.185e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2627, 'grad_norm': 9.828285217285156, 'learning_rate': 1.18e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3518, 'grad_norm': 10.47728443145752, 'learning_rate': 1.175e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2691, 'grad_norm': 9.196660041809082, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3459, 'grad_norm': 8.972760200500488, 'learning_rate': 1.1650000000000002e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4207, 'grad_norm': 10.309515953063965, 'learning_rate': 1.16e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2949, 'grad_norm': 9.871480941772461, 'learning_rate': 1.1550000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3553, 'grad_norm': 10.734230995178223, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3543, 'grad_norm': 10.787270545959473, 'learning_rate': 1.145e-05, 'epoch': 0.27}\n",
      "{'loss': 3.367, 'grad_norm': 8.8077974319458, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3355, 'grad_norm': 10.112235069274902, 'learning_rate': 1.1350000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3813, 'grad_norm': 10.06236743927002, 'learning_rate': 1.13e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4203, 'grad_norm': 9.075162887573242, 'learning_rate': 1.125e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3871, 'grad_norm': 11.215755462646484, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3168, 'grad_norm': 10.267799377441406, 'learning_rate': 1.115e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2838, 'grad_norm': 10.0159912109375, 'learning_rate': 1.11e-05, 'epoch': 0.27}\n",
      "{'loss': 3.235, 'grad_norm': 11.137106895446777, 'learning_rate': 1.1050000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2904, 'grad_norm': 8.940596580505371, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4527, 'grad_norm': 9.645383834838867, 'learning_rate': 1.095e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3676, 'grad_norm': 9.13769817352295, 'learning_rate': 1.09e-05, 'epoch': 0.27}\n",
      "{'loss': 3.343, 'grad_norm': 9.141741752624512, 'learning_rate': 1.0850000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3994, 'grad_norm': 9.378743171691895, 'learning_rate': 1.08e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4477, 'grad_norm': 10.141473770141602, 'learning_rate': 1.075e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2332, 'grad_norm': 9.452768325805664, 'learning_rate': 1.0700000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2812, 'grad_norm': 10.401224136352539, 'learning_rate': 1.065e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2383, 'grad_norm': 10.21823501586914, 'learning_rate': 1.06e-05, 'epoch': 0.28}\n",
      "{'loss': 3.226, 'grad_norm': 9.425212860107422, 'learning_rate': 1.055e-05, 'epoch': 0.28}\n",
      "{'loss': 3.349, 'grad_norm': 11.026390075683594, 'learning_rate': 1.05e-05, 'epoch': 0.28}\n",
      "{'loss': 3.5277, 'grad_norm': 11.09371280670166, 'learning_rate': 1.045e-05, 'epoch': 0.28}\n",
      "{'loss': 3.324, 'grad_norm': 9.11248779296875, 'learning_rate': 1.04e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2572, 'grad_norm': 11.112967491149902, 'learning_rate': 1.035e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3162, 'grad_norm': 10.108262062072754, 'learning_rate': 1.03e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3316, 'grad_norm': 10.068453788757324, 'learning_rate': 1.025e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2729, 'grad_norm': 9.697732925415039, 'learning_rate': 1.02e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4781, 'grad_norm': 10.814976692199707, 'learning_rate': 1.0150000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2934, 'grad_norm': 9.774141311645508, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3398, 'grad_norm': 9.435774803161621, 'learning_rate': 1.005e-05, 'epoch': 0.28}\n",
      "{'loss': 3.241, 'grad_norm': 10.316855430603027, 'learning_rate': 1e-05, 'epoch': 0.28}\n",
      " 80%|██████████████████████████████▍       | 8000/10000 [16:17<26:43,  1.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.89s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:09,  9.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.020292, 'eval_rouge-2': 6.492002, 'eval_rouge-l': 23.70846, 'eval_bleu-4': 0.030092652239533956, 'eval_runtime': 74.2507, 'eval_samples_per_second': 0.673, 'eval_steps_per_second': 0.054, 'epoch': 0.28}\n",
      " 80%|██████████████████████████████▍       | 8000/10000 [17:31<26:43,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 12.59s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-8000\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3242, 'grad_norm': 9.933182716369629, 'learning_rate': 9.950000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 3.2834, 'grad_norm': 9.866487503051758, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.28}\n",
      "{'loss': 3.1986, 'grad_norm': 10.645065307617188, 'learning_rate': 9.85e-06, 'epoch': 0.28}\n",
      "{'loss': 3.333, 'grad_norm': 10.347969055175781, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 3.1939, 'grad_norm': 10.892705917358398, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.28}\n",
      "{'loss': 3.4148, 'grad_norm': 10.315444946289062, 'learning_rate': 9.7e-06, 'epoch': 0.28}\n",
      "{'loss': 3.416, 'grad_norm': 9.616280555725098, 'learning_rate': 9.65e-06, 'epoch': 0.28}\n",
      "{'loss': 3.2523, 'grad_norm': 10.571965217590332, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 3.2707, 'grad_norm': 9.733724594116211, 'learning_rate': 9.55e-06, 'epoch': 0.28}\n",
      "{'loss': 3.3238, 'grad_norm': 10.778339385986328, 'learning_rate': 9.5e-06, 'epoch': 0.28}\n",
      "{'loss': 3.385, 'grad_norm': 9.4298734664917, 'learning_rate': 9.450000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 3.268, 'grad_norm': 9.737981796264648, 'learning_rate': 9.4e-06, 'epoch': 0.28}\n",
      "{'loss': 3.466, 'grad_norm': 10.70616626739502, 'learning_rate': 9.35e-06, 'epoch': 0.28}\n",
      "{'loss': 3.3271, 'grad_norm': 8.650273323059082, 'learning_rate': 9.3e-06, 'epoch': 0.28}\n",
      "{'loss': 3.4025, 'grad_norm': 10.717491149902344, 'learning_rate': 9.25e-06, 'epoch': 0.28}\n",
      "{'loss': 3.2152, 'grad_norm': 10.69356632232666, 'learning_rate': 9.2e-06, 'epoch': 0.28}\n",
      "{'loss': 3.3336, 'grad_norm': 9.489794731140137, 'learning_rate': 9.15e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3439, 'grad_norm': 9.916677474975586, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.4215, 'grad_norm': 10.700126647949219, 'learning_rate': 9.05e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3842, 'grad_norm': 10.288790702819824, 'learning_rate': 9e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3621, 'grad_norm': 9.13318157196045, 'learning_rate': 8.95e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3125, 'grad_norm': 10.503511428833008, 'learning_rate': 8.9e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3002, 'grad_norm': 10.985960960388184, 'learning_rate': 8.85e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3391, 'grad_norm': 9.45643138885498, 'learning_rate': 8.8e-06, 'epoch': 0.29}\n",
      "{'loss': 3.442, 'grad_norm': 10.08956241607666, 'learning_rate': 8.75e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3875, 'grad_norm': 10.793732643127441, 'learning_rate': 8.7e-06, 'epoch': 0.29}\n",
      "{'loss': 3.335, 'grad_norm': 9.410147666931152, 'learning_rate': 8.65e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2207, 'grad_norm': 9.360690116882324, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3807, 'grad_norm': 9.659590721130371, 'learning_rate': 8.550000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.31, 'grad_norm': 9.982373237609863, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3307, 'grad_norm': 10.176712989807129, 'learning_rate': 8.45e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3836, 'grad_norm': 9.213699340820312, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3309, 'grad_norm': 10.256755828857422, 'learning_rate': 8.350000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3879, 'grad_norm': 10.217944145202637, 'learning_rate': 8.3e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2652, 'grad_norm': 9.351778030395508, 'learning_rate': 8.25e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2777, 'grad_norm': 9.957676887512207, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.29, 'grad_norm': 10.28505802154541, 'learning_rate': 8.15e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3295, 'grad_norm': 10.481331825256348, 'learning_rate': 8.1e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2494, 'grad_norm': 10.240926742553711, 'learning_rate': 8.050000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.325, 'grad_norm': 10.804516792297363, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2918, 'grad_norm': 11.43695068359375, 'learning_rate': 7.95e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3592, 'grad_norm': 10.381902694702148, 'learning_rate': 7.9e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3189, 'grad_norm': 9.152451515197754, 'learning_rate': 7.850000000000001e-06, 'epoch': 0.29}\n",
      "{'loss': 3.3748, 'grad_norm': 11.664496421813965, 'learning_rate': 7.8e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2504, 'grad_norm': 8.641597747802734, 'learning_rate': 7.75e-06, 'epoch': 0.29}\n",
      "{'loss': 3.1984, 'grad_norm': 10.43102741241455, 'learning_rate': 7.7e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3264, 'grad_norm': 10.210456848144531, 'learning_rate': 7.65e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3113, 'grad_norm': 9.252084732055664, 'learning_rate': 7.6e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3219, 'grad_norm': 10.20434856414795, 'learning_rate': 7.55e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3494, 'grad_norm': 9.696907043457031, 'learning_rate': 7.5e-06, 'epoch': 0.3}\n",
      " 85%|████████████████████████████████▎     | 8500/10000 [25:01<22:00,  1.14it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.74s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:12, 12.03s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.704518000000004, 'eval_rouge-2': 6.8879019999999995, 'eval_rouge-l': 24.468788, 'eval_bleu-4': 0.034303180966135265, 'eval_runtime': 74.5707, 'eval_samples_per_second': 0.671, 'eval_steps_per_second': 0.054, 'epoch': 0.3}\n",
      " 85%|████████████████████████████████▎     | 8500/10000 [26:15<22:00,  1.14it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 14.26s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-8500\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2287, 'grad_norm': 10.062298774719238, 'learning_rate': 7.45e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3172, 'grad_norm': 9.477677345275879, 'learning_rate': 7.4e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3547, 'grad_norm': 10.132763862609863, 'learning_rate': 7.35e-06, 'epoch': 0.3}\n",
      "{'loss': 3.375, 'grad_norm': 10.845280647277832, 'learning_rate': 7.2999999999999996e-06, 'epoch': 0.3}\n",
      "{'loss': 3.2768, 'grad_norm': 9.010858535766602, 'learning_rate': 7.25e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3115, 'grad_norm': 10.466154098510742, 'learning_rate': 7.2e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3516, 'grad_norm': 10.360712051391602, 'learning_rate': 7.15e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3916, 'grad_norm': 10.383997917175293, 'learning_rate': 7.1e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3088, 'grad_norm': 9.450762748718262, 'learning_rate': 7.049999999999999e-06, 'epoch': 0.3}\n",
      "{'loss': 3.2922, 'grad_norm': 11.619988441467285, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3928, 'grad_norm': 9.770732879638672, 'learning_rate': 6.950000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3396, 'grad_norm': 10.1012601852417, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.4551, 'grad_norm': 9.561206817626953, 'learning_rate': 6.8500000000000005e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3055, 'grad_norm': 10.164340019226074, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3922, 'grad_norm': 9.70178508758545, 'learning_rate': 6.750000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3803, 'grad_norm': 9.883333206176758, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3348, 'grad_norm': 9.744024276733398, 'learning_rate': 6.650000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3182, 'grad_norm': 11.472033500671387, 'learning_rate': 6.6e-06, 'epoch': 0.3}\n",
      "{'loss': 3.2934, 'grad_norm': 10.07514476776123, 'learning_rate': 6.550000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3305, 'grad_norm': 10.327174186706543, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3443, 'grad_norm': 9.396316528320312, 'learning_rate': 6.45e-06, 'epoch': 0.3}\n",
      "{'loss': 3.4438, 'grad_norm': 10.482118606567383, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.3}\n",
      "{'loss': 3.1613, 'grad_norm': 11.07231616973877, 'learning_rate': 6.35e-06, 'epoch': 0.3}\n",
      "{'loss': 3.3277, 'grad_norm': 10.28609561920166, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.4312, 'grad_norm': 11.23196029663086, 'learning_rate': 6.25e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2143, 'grad_norm': 10.57687759399414, 'learning_rate': 6.2e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3514, 'grad_norm': 10.038150787353516, 'learning_rate': 6.15e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2363, 'grad_norm': 9.019238471984863, 'learning_rate': 6.1e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3852, 'grad_norm': 10.18700122833252, 'learning_rate': 6.0500000000000005e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3613, 'grad_norm': 10.26545238494873, 'learning_rate': 6e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2986, 'grad_norm': 10.309556007385254, 'learning_rate': 5.95e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3598, 'grad_norm': 9.512166976928711, 'learning_rate': 5.9e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3457, 'grad_norm': 10.061662673950195, 'learning_rate': 5.850000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3172, 'grad_norm': 10.170910835266113, 'learning_rate': 5.8e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2527, 'grad_norm': 11.063798904418945, 'learning_rate': 5.750000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3719, 'grad_norm': 9.744365692138672, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.31}\n",
      "{'loss': 3.4018, 'grad_norm': 9.776546478271484, 'learning_rate': 5.65e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3566, 'grad_norm': 9.944007873535156, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.3594, 'grad_norm': 9.563566207885742, 'learning_rate': 5.55e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2381, 'grad_norm': 9.473060607910156, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2604, 'grad_norm': 10.273183822631836, 'learning_rate': 5.45e-06, 'epoch': 0.31}\n",
      "{'loss': 3.24, 'grad_norm': 9.890865325927734, 'learning_rate': 5.4e-06, 'epoch': 0.31}\n",
      "{'loss': 3.4146, 'grad_norm': 9.6041259765625, 'learning_rate': 5.3500000000000004e-06, 'epoch': 0.31}\n",
      "{'loss': 3.4355, 'grad_norm': 11.40809154510498, 'learning_rate': 5.3e-06, 'epoch': 0.31}\n",
      "{'loss': 3.4922, 'grad_norm': 11.387557983398438, 'learning_rate': 5.25e-06, 'epoch': 0.31}\n",
      "{'loss': 3.293, 'grad_norm': 9.910078048706055, 'learning_rate': 5.2e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2707, 'grad_norm': 9.959637641906738, 'learning_rate': 5.15e-06, 'epoch': 0.31}\n",
      "{'loss': 3.417, 'grad_norm': 10.292176246643066, 'learning_rate': 5.1e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2557, 'grad_norm': 10.628986358642578, 'learning_rate': 5.050000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.41, 'grad_norm': 11.94222354888916, 'learning_rate': 5e-06, 'epoch': 0.31}\n",
      " 90%|██████████████████████████████████▏   | 9000/10000 [33:49<15:13,  1.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.45s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:11, 11.87s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.124193999999996, 'eval_rouge-2': 6.954887999999999, 'eval_rouge-l': 24.016392, 'eval_bleu-4': 0.03688993483463514, 'eval_runtime': 74.002, 'eval_samples_per_second': 0.676, 'eval_steps_per_second': 0.054, 'epoch': 0.31}\n",
      " 90%|██████████████████████████████████▏   | 9000/10000 [35:03<15:13,  1.09it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 14.16s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-9000\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2781, 'grad_norm': 10.742559432983398, 'learning_rate': 4.950000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.4006, 'grad_norm': 9.443273544311523, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2182, 'grad_norm': 10.391737937927246, 'learning_rate': 4.85e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2775, 'grad_norm': 9.773056983947754, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2424, 'grad_norm': 11.552083015441895, 'learning_rate': 4.75e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3471, 'grad_norm': 9.946956634521484, 'learning_rate': 4.7e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3748, 'grad_norm': 9.228716850280762, 'learning_rate': 4.65e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3719, 'grad_norm': 11.128829956054688, 'learning_rate': 4.6e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2811, 'grad_norm': 10.310831069946289, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3672, 'grad_norm': 11.36167049407959, 'learning_rate': 4.5e-06, 'epoch': 0.32}\n",
      "{'loss': 3.243, 'grad_norm': 11.901813507080078, 'learning_rate': 4.45e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3203, 'grad_norm': 8.790454864501953, 'learning_rate': 4.4e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3625, 'grad_norm': 10.04062557220459, 'learning_rate': 4.35e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2842, 'grad_norm': 9.446259498596191, 'learning_rate': 4.2999999999999995e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2527, 'grad_norm': 9.883493423461914, 'learning_rate': 4.250000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 3.0451, 'grad_norm': 8.846036911010742, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.32}\n",
      "{'loss': 3.4834, 'grad_norm': 10.864364624023438, 'learning_rate': 4.15e-06, 'epoch': 0.32}\n",
      "{'loss': 3.252, 'grad_norm': 10.102397918701172, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.32}\n",
      "{'loss': 3.174, 'grad_norm': 10.55740737915039, 'learning_rate': 4.05e-06, 'epoch': 0.32}\n",
      "{'loss': 3.4262, 'grad_norm': 11.053434371948242, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3285, 'grad_norm': 10.59970760345459, 'learning_rate': 3.95e-06, 'epoch': 0.32}\n",
      "{'loss': 3.4213, 'grad_norm': 10.668822288513184, 'learning_rate': 3.9e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2971, 'grad_norm': 10.817037582397461, 'learning_rate': 3.85e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2039, 'grad_norm': 9.201744079589844, 'learning_rate': 3.8e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3047, 'grad_norm': 11.01651382446289, 'learning_rate': 3.75e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2949, 'grad_norm': 13.678410530090332, 'learning_rate': 3.7e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3961, 'grad_norm': 11.482176780700684, 'learning_rate': 3.6499999999999998e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2723, 'grad_norm': 10.851850509643555, 'learning_rate': 3.6e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2883, 'grad_norm': 11.87056827545166, 'learning_rate': 3.55e-06, 'epoch': 0.32}\n",
      "{'loss': 3.3873, 'grad_norm': 9.65119457244873, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2475, 'grad_norm': 10.78385066986084, 'learning_rate': 3.4500000000000004e-06, 'epoch': 0.32}\n",
      "{'loss': 3.4811, 'grad_norm': 11.045363426208496, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3357, 'grad_norm': 10.665400505065918, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3004, 'grad_norm': 10.89815902709961, 'learning_rate': 3.3e-06, 'epoch': 0.33}\n",
      "{'loss': 3.216, 'grad_norm': 9.728157997131348, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3783, 'grad_norm': 11.43419075012207, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2859, 'grad_norm': 9.871574401855469, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3793, 'grad_norm': 10.443012237548828, 'learning_rate': 3.1e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3875, 'grad_norm': 9.64309024810791, 'learning_rate': 3.05e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2641, 'grad_norm': 11.296260833740234, 'learning_rate': 3e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2395, 'grad_norm': 11.22814655303955, 'learning_rate': 2.95e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2166, 'grad_norm': 9.427035331726074, 'learning_rate': 2.9e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2078, 'grad_norm': 10.997406005859375, 'learning_rate': 2.8500000000000002e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3107, 'grad_norm': 10.751435279846191, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2732, 'grad_norm': 10.137545585632324, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.33}\n",
      "{'loss': 3.368, 'grad_norm': 10.810213088989258, 'learning_rate': 2.7e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2125, 'grad_norm': 8.866578102111816, 'learning_rate': 2.65e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3803, 'grad_norm': 10.569052696228027, 'learning_rate': 2.6e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2408, 'grad_norm': 11.033775329589844, 'learning_rate': 2.55e-06, 'epoch': 0.33}\n",
      "{'loss': 3.1926, 'grad_norm': 10.874192237854004, 'learning_rate': 2.5e-06, 'epoch': 0.33}\n",
      " 95%|████████████████████████████████████  | 9500/10000 [42:30<07:32,  1.10it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.88s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:08,  8.96s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.366567999999997, 'eval_rouge-2': 7.408628, 'eval_rouge-l': 23.915675999999998, 'eval_bleu-4': 0.03396797796351911, 'eval_runtime': 73.267, 'eval_samples_per_second': 0.682, 'eval_steps_per_second': 0.055, 'epoch': 0.33}\n",
      " 95%|████████████████████████████████████  | 9500/10000 [43:44<07:32,  1.10it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:47<00:00, 12.34s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-9500\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4873, 'grad_norm': 11.527182579040527, 'learning_rate': 2.4500000000000003e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2098, 'grad_norm': 9.767741203308105, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2934, 'grad_norm': 9.116296768188477, 'learning_rate': 2.35e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3369, 'grad_norm': 10.628454208374023, 'learning_rate': 2.3e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2824, 'grad_norm': 10.357172966003418, 'learning_rate': 2.25e-06, 'epoch': 0.33}\n",
      "{'loss': 3.3455, 'grad_norm': 10.30418872833252, 'learning_rate': 2.2e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2953, 'grad_norm': 10.464677810668945, 'learning_rate': 2.1499999999999997e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2699, 'grad_norm': 10.932589530944824, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.33}\n",
      "{'loss': 3.5162, 'grad_norm': 10.370429039001465, 'learning_rate': 2.0500000000000003e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2971, 'grad_norm': 10.083686828613281, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3568, 'grad_norm': 9.817910194396973, 'learning_rate': 1.95e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3221, 'grad_norm': 11.425163269042969, 'learning_rate': 1.9e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3645, 'grad_norm': 11.442736625671387, 'learning_rate': 1.85e-06, 'epoch': 0.34}\n",
      "{'loss': 3.2846, 'grad_norm': 11.216812133789062, 'learning_rate': 1.8e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3893, 'grad_norm': 10.6893310546875, 'learning_rate': 1.7500000000000002e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3643, 'grad_norm': 10.861836433410645, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.34}\n",
      "{'loss': 3.2437, 'grad_norm': 9.71420955657959, 'learning_rate': 1.65e-06, 'epoch': 0.34}\n",
      "{'loss': 3.4135, 'grad_norm': 9.296286582946777, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.34}\n",
      "{'loss': 3.2934, 'grad_norm': 9.69701099395752, 'learning_rate': 1.55e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3885, 'grad_norm': 11.38336181640625, 'learning_rate': 1.5e-06, 'epoch': 0.34}\n",
      "{'loss': 3.4023, 'grad_norm': 10.427702903747559, 'learning_rate': 1.45e-06, 'epoch': 0.34}\n",
      "{'loss': 3.4346, 'grad_norm': 10.783794403076172, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3809, 'grad_norm': 10.997767448425293, 'learning_rate': 1.35e-06, 'epoch': 0.34}\n",
      "{'loss': 3.4682, 'grad_norm': 10.24560832977295, 'learning_rate': 1.3e-06, 'epoch': 0.34}\n",
      "{'loss': 3.376, 'grad_norm': 11.858380317687988, 'learning_rate': 1.25e-06, 'epoch': 0.34}\n",
      "{'loss': 3.4174, 'grad_norm': 9.50980281829834, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3615, 'grad_norm': 10.456470489501953, 'learning_rate': 1.15e-06, 'epoch': 0.34}\n",
      "{'loss': 3.2658, 'grad_norm': 10.370131492614746, 'learning_rate': 1.1e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3715, 'grad_norm': 9.830074310302734, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.34}\n",
      "{'loss': 3.1736, 'grad_norm': 10.444035530090332, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.34}\n",
      "{'loss': 3.3609, 'grad_norm': 10.856335639953613, 'learning_rate': 9.5e-07, 'epoch': 0.34}\n",
      "{'loss': 3.1902, 'grad_norm': 10.116742134094238, 'learning_rate': 9e-07, 'epoch': 0.34}\n",
      "{'loss': 3.2727, 'grad_norm': 11.250994682312012, 'learning_rate': 8.500000000000001e-07, 'epoch': 0.34}\n",
      "{'loss': 3.2387, 'grad_norm': 9.785406112670898, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.34}\n",
      "{'loss': 3.0447, 'grad_norm': 10.45068359375, 'learning_rate': 7.5e-07, 'epoch': 0.34}\n",
      "{'loss': 3.1738, 'grad_norm': 10.53233814239502, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.34}\n",
      "{'loss': 3.4979, 'grad_norm': 11.469992637634277, 'learning_rate': 6.5e-07, 'epoch': 0.34}\n",
      "{'loss': 3.275, 'grad_norm': 9.892984390258789, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.34}\n",
      "{'loss': 3.299, 'grad_norm': 9.128741264343262, 'learning_rate': 5.5e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3545, 'grad_norm': 11.780360221862793, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3217, 'grad_norm': 9.661582946777344, 'learning_rate': 4.5e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3623, 'grad_norm': 12.592399597167969, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.35}\n",
      "{'loss': 3.2912, 'grad_norm': 9.734030723571777, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3871, 'grad_norm': 10.12063980102539, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3986, 'grad_norm': 9.265460014343262, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3314, 'grad_norm': 12.09356689453125, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3314, 'grad_norm': 10.75014877319336, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.35}\n",
      "{'loss': 3.2957, 'grad_norm': 11.150362968444824, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.35}\n",
      "{'loss': 3.3527, 'grad_norm': 10.614835739135742, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.35}\n",
      "{'loss': 3.0934, 'grad_norm': 9.552786827087402, 'learning_rate': 0.0, 'epoch': 0.35}\n",
      "100%|█████████████████████████████████████| 10000/10000 [51:15<00:00,  1.04it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.87s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.94s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.976948, 'eval_rouge-2': 7.8810080000000005, 'eval_rouge-l': 24.214358, 'eval_bleu-4': 0.0379806758704705, 'eval_runtime': 79.3798, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.05, 'epoch': 0.35}\n",
      "100%|█████████████████████████████████████| 10000/10000 [52:35<00:00,  1.04it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.21s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /home/abc/mydisk/model/chatglm3-6b/output/checkpoint-10000\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/abc/mydisk/model/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3155.1375, 'train_samples_per_second': 12.678, 'train_steps_per_second': 3.169, 'train_loss': 0.997874609375, 'epoch': 0.35}\n",
      "100%|█████████████████████████████████████| 10000/10000 [52:35<00:00,  3.17it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [19:37<00:00, 17.57s/it]\n"
     ]
    }
   ],
   "source": [
    "! python3 /home/abc/mydisk/ChatGLM3/finetune_demo/finetune_hf.py /home/abc/mydisk/ChatGLM3/finetune_demo/data/ /home/abc/mydisk/model/chatglm3-6b/ /home/abc/mydisk/ChatGLM3/finetune_demo/configs/lora.yaml YES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c023f70-88d0-4b3d-87c0-f298786677bc",
   "metadata": {},
   "source": [
    "已完成10000 steps的训练，约0.35 epoch。\n",
    "train loss整体在3.4以下，约有一半概率在3.3以下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da420b0-ec2a-4d97-8b7b-b67f347003c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 22:41:20 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10                     Off |   00000000:00:08.0 Off |                    0 |\n",
      "|  0%   33C    P8             17W /  150W |      14MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1938      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a54d4-4a38-422b-ad01-630654939801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
